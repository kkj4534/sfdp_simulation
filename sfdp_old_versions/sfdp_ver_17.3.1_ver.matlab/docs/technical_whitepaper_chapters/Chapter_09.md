# Chapter 9: Machine Learning Implementation

## 9.1 Empirical Model Suite Architecture

The SFDP system incorporates machine learning through its empirical model suite, which provides data-driven corrections and predictions to complement the physics-based calculations.

### 9.1.1 Neural Network Implementations

**File: SFDP_empirical_ml_suite.m:1-100**

```matlab
function [ml_results, ml_confidence] = performAdvancedMachineLearning(cutting_conditions, ...
    material_props, physics_results, simulation_state)
    
    fprintf('ğŸ¤– ê³ ê¸‰ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì‹¤í–‰ ì‹œì‘\n');
    
    ml_results = struct();
    ml_confidence = 0;
    
    try
        % 1. ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬
        fprintf('  ğŸ“Š ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\n');
        [processed_features, feature_confidence] = preprocessMLFeatures(...
            cutting_conditions, material_props, physics_results, simulation_state);
        
        if feature_confidence < 0.5
            warning('íŠ¹ì„± ì¶”ì¶œ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤ (%.2f)', feature_confidence);
        end
        
        % 2. ì‹ ê²½ë§ ê¸°ë°˜ ì˜¨ë„ ì˜ˆì¸¡
        fprintf('  ğŸŒ¡ï¸ ì‹ ê²½ë§ ì˜¨ë„ ì˜ˆì¸¡ ì‹¤í–‰ ì¤‘...\n');
        [temperature_prediction, temp_confidence] = neuralNetworkTemperaturePrediction(...
            processed_features, simulation_state);
        
        ml_results.temperature = temperature_prediction;
        ml_results.temperature_confidence = temp_confidence;
        
        % 3. ëœë¤ í¬ë ˆìŠ¤íŠ¸ ê³µêµ¬ë§ˆëª¨ ì˜ˆì¸¡
        fprintf('  ğŸ”§ ëœë¤ í¬ë ˆìŠ¤íŠ¸ ê³µêµ¬ë§ˆëª¨ ì˜ˆì¸¡ ì‹¤í–‰ ì¤‘...\n');
        [wear_prediction, wear_confidence] = randomForestWearPrediction(...
            processed_features, simulation_state);
        
        ml_results.tool_wear = wear_prediction;
        ml_results.wear_confidence = wear_confidence;
        
        % 4. SVM í‘œë©´ì¡°ë„ ì˜ˆì¸¡
        fprintf('  ğŸ“ SVM í‘œë©´ì¡°ë„ ì˜ˆì¸¡ ì‹¤í–‰ ì¤‘...\n');
        [surface_prediction, surface_confidence] = svmSurfaceRoughnessPrediction(...
            processed_features, simulation_state);
        
        ml_results.surface_roughness = surface_prediction;
        ml_results.surface_confidence = surface_confidence;
        
        % ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°
        confidences = [temp_confidence, wear_confidence, surface_confidence];
        ml_confidence = mean(confidences(confidences > 0));
        
        fprintf('  âœ… ë¨¸ì‹ ëŸ¬ë‹ ì˜ˆì¸¡ ì™„ë£Œ (ì „ì²´ ì‹ ë¢°ë„: %.2f)\n', ml_confidence);
        
    catch ME
        fprintf('  âŒ ë¨¸ì‹ ëŸ¬ë‹ ì‹¤í–‰ ì‹¤íŒ¨: %s\n', ME.message);
        ml_results = [];
        ml_confidence = 0;
    end
end
```

### 9.1.2 Feature Engineering and Data Preprocessing

The system implements sophisticated feature engineering to extract meaningful patterns from the machining data.

**Feature Extraction Implementation (SFDP_empirical_ml_suite.m:200-350)**

```matlab
function [processed_features, feature_confidence] = preprocessMLFeatures(cutting_conditions, ...
    material_props, physics_results, simulation_state)
    
    processed_features = struct();
    feature_confidence = 0;
    
    try
        % 1. ê¸°ë³¸ ì ˆì‚­ ì¡°ê±´ íŠ¹ì„±
        basic_features = extractBasicCuttingFeatures(cutting_conditions);
        
        % 2. ì¬ë£Œ ì†ì„± íŠ¹ì„±
        material_features = extractMaterialPropertyFeatures(material_props);
        
        % 3. ë¬¼ë¦¬ ê¸°ë°˜ íŠ¹ì„±
        physics_features = extractPhysicsBasedFeatures(physics_results);
        
        % 4. ìƒí˜¸ì‘ìš© íŠ¹ì„±
        interaction_features = createInteractionFeatures(basic_features, material_features);
        
        % 5. ì‹œê°„ ë„ë©”ì¸ íŠ¹ì„±
        temporal_features = extractTemporalFeatures(cutting_conditions, simulation_state);
        
        % íŠ¹ì„± ê²°í•©
        processed_features.basic = basic_features;
        processed_features.material = material_features;
        processed_features.physics = physics_features;
        processed_features.interaction = interaction_features;
        processed_features.temporal = temporal_features;
        
        % íŠ¹ì„± ì •ê·œí™”
        processed_features.normalized = normalizeFeatures(processed_features, simulation_state);
        
        % íŠ¹ì„± ì„ íƒ
        processed_features.selected = selectOptimalFeatures(processed_features.normalized, simulation_state);
        
        feature_confidence = evaluateFeatureQuality(processed_features);
        
    catch ME
        fprintf('íŠ¹ì„± ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: %s\n', ME.message);
        processed_features = [];
        feature_confidence = 0;
    end
end

function basic_features = extractBasicCuttingFeatures(cutting_conditions)
    basic_features = struct();
    
    % ì§ì ‘ ì ˆì‚­ íŒŒë¼ë¯¸í„°
    basic_features.cutting_speed = cutting_conditions.speed;          % m/min
    basic_features.feed_rate = cutting_conditions.feed;              % mm/rev
    basic_features.depth_of_cut = cutting_conditions.depth;          % mm
    
    % ê³„ì‚°ëœ íŒŒë¼ë¯¸í„°
    if isfield(cutting_conditions, 'tool') && isfield(cutting_conditions.tool, 'diameter')
        tool_diameter = cutting_conditions.tool.diameter;
        basic_features.spindle_speed = (cutting_conditions.speed * 1000) / (pi * tool_diameter); % RPM
    else
        basic_features.spindle_speed = cutting_conditions.speed * 318.3; % ê¸°ë³¸ ì¶”ì •ê°’
    end
    
    % ì¬ë£Œ ì œê±°ìœ¨
    basic_features.material_removal_rate = cutting_conditions.speed * cutting_conditions.feed * cutting_conditions.depth;
    
    % ë¬´ì°¨ì› ìˆ˜
    basic_features.speed_to_feed_ratio = cutting_conditions.speed / cutting_conditions.feed;
    basic_features.feed_to_depth_ratio = cutting_conditions.feed / cutting_conditions.depth;
    
    % ë¡œê·¸ ë³€í™˜ íŠ¹ì„±
    basic_features.log_speed = log(cutting_conditions.speed);
    basic_features.log_feed = log(cutting_conditions.feed);
    basic_features.log_depth = log(cutting_conditions.depth);
    
    % ì œê³±ê·¼ ë³€í™˜ íŠ¹ì„±
    basic_features.sqrt_speed = sqrt(cutting_conditions.speed);
    basic_features.sqrt_feed = sqrt(cutting_conditions.feed);
    basic_features.sqrt_depth = sqrt(cutting_conditions.depth);
end

function material_features = extractMaterialPropertyFeatures(material_props)
    material_features = struct();
    
    % ì—´ì  ì†ì„±
    if isfield(material_props, 'thermal')
        material_features.thermal_conductivity = material_props.thermal.conductivity;
        material_features.specific_heat = material_props.thermal.specific_heat;
        material_features.density = material_props.thermal.density;
        material_features.thermal_diffusivity = material_props.thermal.conductivity / ...
            (material_props.thermal.density * material_props.thermal.specific_heat);
    end
    
    % ê¸°ê³„ì  ì†ì„±
    if isfield(material_props, 'mechanical')
        material_features.youngs_modulus = material_props.mechanical.youngs_modulus;
        material_features.yield_strength = material_props.mechanical.yield_strength;
        material_features.hardness = material_props.mechanical.hardness_hv;
        material_features.poisson_ratio = material_props.mechanical.poisson_ratio;
    end
    
    % ê°€ê³µì„± ì§€ìˆ˜
    if isfield(material_props, 'machinability')
        material_features.machinability_index = material_props.machinability.tool_wear_factor;
        material_features.cutting_force_coefficient = material_props.machinability.cutting_force_coefficient;
    end
    
    % íŒŒìƒ íŠ¹ì„±
    if isfield(material_features, 'yield_strength') && isfield(material_features, 'hardness')
        material_features.strength_hardness_ratio = material_features.yield_strength / (material_features.hardness * 1e6);
    end
end

function physics_features = extractPhysicsBasedFeatures(physics_results)
    physics_features = struct();
    
    if isempty(physics_results)
        % ë¬¼ë¦¬ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’
        physics_features.max_temperature = 0;
        physics_features.avg_temperature = 0;
        physics_features.temperature_gradient = 0;
        physics_features.max_stress = 0;
        physics_features.contact_pressure = 0;
        return;
    end
    
    % ì—´ì  íŠ¹ì„±
    if isfield(physics_results, 'thermal')
        if isfield(physics_results.thermal, 'max_temperature')
            physics_features.max_temperature = physics_results.thermal.max_temperature;
        end
        if isfield(physics_results.thermal, 'avg_temperature')
            physics_features.avg_temperature = physics_results.thermal.avg_temperature;
        end
        if isfield(physics_results.thermal, 'max_gradient')
            physics_features.temperature_gradient = physics_results.thermal.max_gradient;
        end
    end
    
    % ê¸°ê³„ì  íŠ¹ì„±
    if isfield(physics_results, 'mechanical')
        if isfield(physics_results.mechanical, 'max_stress')
            physics_features.max_stress = physics_results.mechanical.max_stress;
        end
        if isfield(physics_results.mechanical, 'avg_pressure')
            physics_features.contact_pressure = physics_results.mechanical.avg_pressure;
        end
    end
    
    % ë§ˆëª¨ íŠ¹ì„±
    if isfield(physics_results, 'wear')
        if isfield(physics_results.wear, 'total_wear')
            physics_features.predicted_wear = physics_results.wear.total_wear;
        end
        if isfield(physics_results.wear, 'wear_rate')
            physics_features.wear_rate = physics_results.wear.wear_rate;
        end
    end
end

function interaction_features = createInteractionFeatures(basic_features, material_features)
    interaction_features = struct();
    
    % ì ˆì‚­ ì¡°ê±´ê³¼ ì¬ë£Œ ì†ì„±ì˜ ìƒí˜¸ì‘ìš©
    if isfield(basic_features, 'cutting_speed') && isfield(material_features, 'thermal_conductivity')
        interaction_features.speed_thermal_interaction = basic_features.cutting_speed / material_features.thermal_conductivity;
    end
    
    if isfield(basic_features, 'feed_rate') && isfield(material_features, 'hardness')
        interaction_features.feed_hardness_interaction = basic_features.feed_rate * material_features.hardness;
    end
    
    if isfield(basic_features, 'material_removal_rate') && isfield(material_features, 'density')
        interaction_features.mrr_density_interaction = basic_features.material_removal_rate * material_features.density;
    end
    
    % ë³µí•© ë¬´ì°¨ì› ìˆ˜
    if isfield(basic_features, 'cutting_speed') && isfield(material_features, 'thermal_diffusivity')
        characteristic_length = 1e-3;  % 1mm íŠ¹ì„± ê¸¸ì´
        interaction_features.peclet_number = (basic_features.cutting_speed / 60) * characteristic_length / material_features.thermal_diffusivity;
    end
end
```

### 9.1.3 Neural Network Temperature Prediction

The system uses a deep neural network for temperature prediction, incorporating both cutting conditions and material properties.

**Neural Network Implementation (SFDP_empirical_ml_suite.m:500-700)**

```matlab
function [temperature_prediction, temp_confidence] = neuralNetworkTemperaturePrediction(...
    processed_features, simulation_state)
    
    temperature_prediction = struct();
    temp_confidence = 0;
    
    try
        % 1. ì‹ ê²½ë§ ì…ë ¥ ë²¡í„° êµ¬ì„±
        input_vector = constructNeuralNetworkInput(processed_features);
        
        % 2. ì‹ ê²½ë§ ëª¨ë¸ ë¡œë“œ ë˜ëŠ” ìƒì„±
        if isfield(simulation_state, 'ml_models') && isfield(simulation_state.ml_models, 'temperature_nn')
            % ê¸°ì¡´ í›ˆë ¨ëœ ëª¨ë¸ ì‚¬ìš©
            nn_model = simulation_state.ml_models.temperature_nn;
        else
            % ê¸°ë³¸ ì‹ ê²½ë§ êµ¬ì¡° ìƒì„±
            nn_model = createDefaultTemperatureNeuralNetwork();
        end
        
        % 3. ìˆœì „íŒŒ ì˜ˆì¸¡
        [nn_output, prediction_uncertainty] = forwardPrediction(nn_model, input_vector);
        
        % 4. ê²°ê³¼ í•´ì„ ë° í›„ì²˜ë¦¬
        temperature_prediction.peak_temperature = nn_output(1) * simulation_state.scaling.temperature_max;
        temperature_prediction.average_temperature = nn_output(2) * simulation_state.scaling.temperature_max;
        temperature_prediction.temperature_gradient = nn_output(3) * simulation_state.scaling.gradient_max;
        
        % ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”
        temperature_prediction.uncertainty = struct();
        temperature_prediction.uncertainty.epistemic = prediction_uncertainty.model_uncertainty;
        temperature_prediction.uncertainty.aleatoric = prediction_uncertainty.data_uncertainty;
        temperature_prediction.uncertainty.total = sqrt(prediction_uncertainty.model_uncertainty^2 + ...
                                                        prediction_uncertainty.data_uncertainty^2);
        
        % ì‹ ë¢°ë„ ê³„ì‚°
        temp_confidence = calculatePredictionConfidence(temperature_prediction, nn_model);
        
    catch ME
        fprintf('ì‹ ê²½ë§ ì˜¨ë„ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: %s\n', ME.message);
        temperature_prediction = [];
        temp_confidence = 0;
    end
end

function nn_model = createDefaultTemperatureNeuralNetwork()
    % ê¸°ë³¸ ì‹ ê²½ë§ êµ¬ì¡° ì •ì˜
    nn_model = struct();
    
    % ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°
    nn_model.architecture = [15, 20, 15, 10, 3]; % ì…ë ¥-ì€ë‹‰ì¸µë“¤-ì¶œë ¥
    nn_model.activation_functions = {'relu', 'relu', 'relu', 'linear'}; % ë ˆì´ì–´ë³„ í™œì„±í™” í•¨ìˆ˜
    
    % ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (ê¸°ë³¸ê°’ - ì‹¤ì œë¡œëŠ” í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©)
    nn_model.weights = cell(1, length(nn_model.architecture)-1);
    nn_model.biases = cell(1, length(nn_model.architecture)-1);
    
    for i = 1:length(nn_model.architecture)-1
        % Xavier ì´ˆê¸°í™”
        fan_in = nn_model.architecture(i);
        fan_out = nn_model.architecture(i+1);
        xavier_bound = sqrt(6.0 / (fan_in + fan_out));
        
        nn_model.weights{i} = (rand(fan_out, fan_in) * 2 - 1) * xavier_bound;
        nn_model.biases{i} = zeros(fan_out, 1);
    end
    
    % í›ˆë ¨ ê´€ë ¨ ì •ë³´
    nn_model.training_info = struct();
    nn_model.training_info.epochs_trained = 0;
    nn_model.training_info.validation_loss = inf;
    nn_model.training_info.regularization = 0.001;
end

function [output, uncertainty] = forwardPrediction(nn_model, input_vector)
    % ì‹ ê²½ë§ ìˆœì „íŒŒ ì˜ˆì¸¡
    
    current_activation = input_vector(:);
    
    % ê° ë ˆì´ì–´ë¥¼ í†µê³¼
    for layer = 1:length(nn_model.weights)
        % ì„ í˜• ë³€í™˜
        z = nn_model.weights{layer} * current_activation + nn_model.biases{layer};
        
        % í™œì„±í™” í•¨ìˆ˜ ì ìš©
        activation_func = nn_model.activation_functions{layer};
        switch activation_func
            case 'relu'
                current_activation = max(0, z);
            case 'sigmoid'
                current_activation = 1 ./ (1 + exp(-z));
            case 'tanh'
                current_activation = tanh(z);
            case 'linear'
                current_activation = z;
            otherwise
                current_activation = z;
        end
    end
    
    output = current_activation;
    
    % ë¶ˆí™•ì‹¤ì„± ì¶”ì • (ê°„ì†Œí™”ëœ ë²„ì „)
    uncertainty = struct();
    uncertainty.model_uncertainty = 0.1 * norm(output);  % ëª¨ë¸ ë¶ˆí™•ì‹¤ì„±
    uncertainty.data_uncertainty = 0.05 * norm(output);  % ë°ì´í„° ë¶ˆí™•ì‹¤ì„±
end

function confidence = calculatePredictionConfidence(prediction, nn_model)
    % ì˜ˆì¸¡ ì‹ ë¢°ë„ ê³„ì‚°
    
    confidence = 0.5;  % ê¸°ë³¸ê°’
    
    try
        % ë¶ˆí™•ì‹¤ì„± ê¸°ë°˜ ì‹ ë¢°ë„
        if isfield(prediction, 'uncertainty') && isfield(prediction.uncertainty, 'total')
            total_uncertainty = prediction.uncertainty.total;
            max_reasonable_uncertainty = 100;  % ì˜ˆìƒ ìµœëŒ€ ë¶ˆí™•ì‹¤ì„±
            
            uncertainty_score = max(0, 1 - total_uncertainty / max_reasonable_uncertainty);
            confidence = confidence * 0.5 + uncertainty_score * 0.5;
        end
        
        % ë¬¼ë¦¬ì  íƒ€ë‹¹ì„± ê¸°ë°˜ ì‹ ë¢°ë„
        if isfield(prediction, 'peak_temperature')
            temp = prediction.peak_temperature;
            if temp > 0 && temp < 2000  % í•©ë¦¬ì  ì˜¨ë„ ë²”ìœ„
                physics_score = 1.0;
            else
                physics_score = 0.2;
            end
            confidence = confidence * 0.7 + physics_score * 0.3;
        end
        
        % ëª¨ë¸ í›ˆë ¨ í’ˆì§ˆ ë°˜ì˜
        if isfield(nn_model, 'training_info') && isfield(nn_model.training_info, 'validation_loss')
            if nn_model.training_info.validation_loss < 0.1
                training_score = 0.9;
            elseif nn_model.training_info.validation_loss < 0.5
                training_score = 0.7;
            else
                training_score = 0.4;
            end
            confidence = confidence * 0.8 + training_score * 0.2;
        end
        
    catch ME
        fprintf('ì‹ ë¢°ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: %s\n', ME.message);
        confidence = 0.3;
    end
    
    % ì‹ ë¢°ë„ ë²”ìœ„ ì œí•œ
    confidence = max(0.1, min(0.95, confidence));
end
```

## 9.2 Support Vector Machine and Random Forest Models

### 9.2.1 SVM Surface Roughness Prediction

**Implementation (SFDP_empirical_ml_suite.m:800-950)**

```matlab
function [surface_prediction, surface_confidence] = svmSurfaceRoughnessPrediction(...
    processed_features, simulation_state)
    
    surface_prediction = struct();
    surface_confidence = 0;
    
    try
        % 1. SVM ì…ë ¥ ë²¡í„° êµ¬ì„±
        svm_input = constructSVMInput(processed_features);
        
        % 2. SVM ëª¨ë¸ ì„¤ì •
        if isfield(simulation_state, 'ml_models') && isfield(simulation_state.ml_models, 'surface_svm')
            svm_model = simulation_state.ml_models.surface_svm;
        else
            svm_model = createDefaultSurfaceSVM();
        end
        
        % 3. SVM ì˜ˆì¸¡ ì‹¤í–‰
        [svm_output, decision_values] = predictSVM(svm_model, svm_input);
        
        % 4. ê²°ê³¼ í•´ì„
        surface_prediction.ra_roughness = svm_output(1) * simulation_state.scaling.roughness_max;
        surface_prediction.rz_roughness = svm_output(2) * simulation_state.scaling.roughness_max;
        surface_prediction.roughness_profile = svm_output(3:end);
        
        % ì‹ ë¢°ë„ ê³„ì‚° (ê²°ì • ê²½ê³„ë¡œë¶€í„°ì˜ ê±°ë¦¬ ê¸°ë°˜)
        surface_confidence = calculateSVMConfidence(decision_values, svm_model);
        
    catch ME
        fprintf('SVM í‘œë©´ì¡°ë„ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: %s\n', ME.message);
        surface_prediction = [];
        surface_confidence = 0;
    end
end

function svm_model = createDefaultSurfaceSVM()
    svm_model = struct();
    
    % SVM í•˜ì´í¼íŒŒë¼ë¯¸í„°
    svm_model.kernel = 'rbf';           % ë°©ì‚¬ê¸°ì €í•¨ìˆ˜ ì»¤ë„
    svm_model.gamma = 0.1;              % RBF ì»¤ë„ íŒŒë¼ë¯¸í„°
    svm_model.C = 1.0;                  % ì •ê·œí™” íŒŒë¼ë¯¸í„°
    svm_model.epsilon = 0.01;           % SVRì˜ epsilon íŒŒë¼ë¯¸í„°
    
    % ì§€ì› ë²¡í„° (ê¸°ë³¸ê°’ - ì‹¤ì œë¡œëŠ” í›ˆë ¨ëœ ê°’ ì‚¬ìš©)
    svm_model.support_vectors = randn(50, 15); % 50ê°œ ì§€ì›ë²¡í„°, 15ì°¨ì› íŠ¹ì„±
    svm_model.support_vector_labels = randn(50, 3); % 3ì°¨ì› ì¶œë ¥
    svm_model.alphas = randn(50, 3);    % ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜
    svm_model.b = randn(3, 1);          % í¸í–¥
    
    % ì •ê·œí™” íŒŒë¼ë¯¸í„°
    svm_model.input_mean = zeros(15, 1);
    svm_model.input_std = ones(15, 1);
    svm_model.output_mean = zeros(3, 1);
    svm_model.output_std = ones(3, 1);
end

function [output, decision_values] = predictSVM(svm_model, input_vector)
    % SVM ì˜ˆì¸¡ ì‹¤í–‰
    
    % ì…ë ¥ ì •ê·œí™”
    normalized_input = (input_vector - svm_model.input_mean) ./ svm_model.input_std;
    
    % ì»¤ë„ í•¨ìˆ˜ ê³„ì‚°
    if strcmp(svm_model.kernel, 'rbf')
        % RBF ì»¤ë„
        distances = sum((svm_model.support_vectors - normalized_input').^2, 2);
        kernel_values = exp(-svm_model.gamma * distances);
    else
        % ì„ í˜• ì»¤ë„ (í´ë°±)
        kernel_values = svm_model.support_vectors * normalized_input;
    end
    
    % ì˜ˆì¸¡ê°’ ê³„ì‚°
    raw_output = zeros(size(svm_model.alphas, 2), 1);
    for i = 1:size(svm_model.alphas, 2)
        raw_output(i) = sum(svm_model.alphas(:, i) .* kernel_values) + svm_model.b(i);
    end
    
    % ì¶œë ¥ ì—­ì •ê·œí™”
    output = raw_output .* svm_model.output_std + svm_model.output_mean;
    
    % ê²°ì • ê°’ (ì‹ ë¢°ë„ ê³„ì‚°ìš©)
    decision_values = abs(raw_output);
end

function confidence = calculateSVMConfidence(decision_values, svm_model)
    % SVM ì‹ ë¢°ë„ ê³„ì‚°
    
    % ê²°ì • ê²½ê³„ë¡œë¶€í„°ì˜ ê±°ë¦¬ ê¸°ë°˜
    min_decision_value = min(abs(decision_values));
    max_decision_value = max(abs(decision_values));
    
    % ì •ê·œí™”ëœ ì‹ ë¢°ë„
    if max_decision_value > 0
        confidence = min_decision_value / max_decision_value;
    else
        confidence = 0.5;
    end
    
    % ëª¨ë¸ í’ˆì§ˆ ë°˜ì˜
    if isfield(svm_model, 'validation_score')
        confidence = confidence * 0.7 + svm_model.validation_score * 0.3;
    end
    
    confidence = max(0.1, min(0.9, confidence));
end
```

### 9.2.2 Random Forest Tool Wear Prediction

**Implementation (SFDP_empirical_ml_suite.m:1100-1300)**

```matlab
function [wear_prediction, wear_confidence] = randomForestWearPrediction(...
    processed_features, simulation_state)
    
    wear_prediction = struct();
    wear_confidence = 0;
    
    try
        % 1. ëœë¤ í¬ë ˆìŠ¤íŠ¸ ì…ë ¥ êµ¬ì„±
        rf_input = constructRandomForestInput(processed_features);
        
        % 2. ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ ë¡œë“œ
        if isfield(simulation_state, 'ml_models') && isfield(simulation_state.ml_models, 'wear_rf')
            rf_model = simulation_state.ml_models.wear_rf;
        else
            rf_model = createDefaultWearRandomForest();
        end
        
        % 3. ì•™ìƒë¸” ì˜ˆì¸¡ ì‹¤í–‰
        [rf_predictions, prediction_variance] = predictRandomForest(rf_model, rf_input);
        
        % 4. ê²°ê³¼ í•´ì„
        wear_prediction.flank_wear = rf_predictions(1) * simulation_state.scaling.wear_max;
        wear_prediction.crater_wear = rf_predictions(2) * simulation_state.scaling.wear_max;
        wear_prediction.tool_life = rf_predictions(3) * simulation_state.scaling.life_max;
        
        % ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„±
        wear_prediction.uncertainty = struct();
        wear_prediction.uncertainty.flank_wear_std = sqrt(prediction_variance(1));
        wear_prediction.uncertainty.crater_wear_std = sqrt(prediction_variance(2));
        wear_prediction.uncertainty.tool_life_std = sqrt(prediction_variance(3));
        
        % ì‹ ë¢°ë„ ê³„ì‚° (ì˜ˆì¸¡ ë¶„ì‚°ì˜ ì—­ìˆ˜ ê¸°ë°˜)
        wear_confidence = calculateRandomForestConfidence(prediction_variance, rf_model);
        
    catch ME
        fprintf('ëœë¤ í¬ë ˆìŠ¤íŠ¸ ê³µêµ¬ë§ˆëª¨ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: %s\n', ME.message);
        wear_prediction = [];
        wear_confidence = 0;
    end
end

function rf_model = createDefaultWearRandomForest()
    rf_model = struct();
    
    % ëœë¤ í¬ë ˆìŠ¤íŠ¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    rf_model.n_trees = 100;             % íŠ¸ë¦¬ ê°œìˆ˜
    rf_model.max_depth = 10;            % ìµœëŒ€ ê¹Šì´
    rf_model.min_samples_split = 5;     % ë¶„í•  ìµœì†Œ ìƒ˜í”Œ
    rf_model.min_samples_leaf = 2;      % ì ë…¸ë“œ ìµœì†Œ ìƒ˜í”Œ
    rf_model.max_features = 'sqrt';     % íŠ¹ì„± ì„ íƒ ë°©ë²•
    
    % ê°œë³„ íŠ¸ë¦¬ë“¤ (ê°„ì†Œí™”ëœ êµ¬ì¡°)
    rf_model.trees = cell(1, rf_model.n_trees);
    for i = 1:rf_model.n_trees
        tree = struct();
        tree.tree_id = i;
        tree.feature_indices = randperm(15, 4); % 15ê°œ ì¤‘ 4ê°œ íŠ¹ì„± ì„ íƒ
        tree.thresholds = randn(1, 4);
        tree.leaf_values = randn(8, 3);         % 8ê°œ ìë…¸ë“œ, 3ì°¨ì› ì¶œë ¥
        rf_model.trees{i} = tree;
    end
    
    % íŠ¹ì„± ì¤‘ìš”ë„
    rf_model.feature_importance = rand(15, 1);
    rf_model.feature_importance = rf_model.feature_importance / sum(rf_model.feature_importance);
end

function [predictions, variance] = predictRandomForest(rf_model, input_vector)
    % ëœë¤ í¬ë ˆìŠ¤íŠ¸ ì˜ˆì¸¡
    
    n_trees = rf_model.n_trees;
    n_outputs = 3;  % flank wear, crater wear, tool life
    
    tree_predictions = zeros(n_trees, n_outputs);
    
    % ê° íŠ¸ë¦¬ì—ì„œ ì˜ˆì¸¡
    for i = 1:n_trees
        tree = rf_model.trees{i};
        tree_output = predictSingleTree(tree, input_vector);
        tree_predictions(i, :) = tree_output;
    end
    
    % ì•™ìƒë¸” í‰ê· 
    predictions = mean(tree_predictions, 1);
    
    % ì˜ˆì¸¡ ë¶„ì‚° (ë¶ˆí™•ì‹¤ì„±)
    variance = var(tree_predictions, [], 1);
end

function output = predictSingleTree(tree, input_vector)
    % ë‹¨ì¼ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ ì˜ˆì¸¡ (ê°„ì†Œí™”ëœ ë²„ì „)
    
    selected_features = input_vector(tree.feature_indices);
    
    % ê°„ë‹¨í•œ ì„ í˜• ê²°í•© (ì‹¤ì œë¡œëŠ” íŠ¸ë¦¬ êµ¬ì¡° ì‚¬ìš©)
    output = sum(selected_features .* tree.thresholds') + tree.leaf_values(1, :);
    
    % ë¬¼ë¦¬ì  ì œì•½ ì ìš©
    output = max(0, output);  % ìŒìˆ˜ ë°©ì§€
end

function confidence = calculateRandomForestConfidence(prediction_variance, rf_model)
    % ëœë¤ í¬ë ˆìŠ¤íŠ¸ ì‹ ë¢°ë„ ê³„ì‚°
    
    % ì˜ˆì¸¡ ë¶„ì‚°ì´ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ì‹ ë¢°ë„
    avg_variance = mean(prediction_variance);
    max_expected_variance = 0.1;  % ì˜ˆìƒ ìµœëŒ€ ë¶„ì‚°
    
    variance_score = max(0, 1 - avg_variance / max_expected_variance);
    
    % íŠ¸ë¦¬ ê°œìˆ˜ íš¨ê³¼
    tree_score = min(1, rf_model.n_trees / 100);  % 100ê°œ íŠ¸ë¦¬ê°€ ìµœì 
    
    % ì¢…í•© ì‹ ë¢°ë„
    confidence = variance_score * 0.7 + tree_score * 0.3;
    
    confidence = max(0.1, min(0.9, confidence));
end
```

## 9.3 Model Training and Validation Framework

### 9.3.1 Cross-Validation Implementation

The system includes comprehensive model validation to ensure robustness and generalization.

**Validation Framework (SFDP_empirical_ml_suite.m:1500-1700)**

```matlab
function [validation_results, model_performance] = performMLModelValidation(...
    training_data, validation_data, simulation_state)
    
    fprintf('ğŸ§ª ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ê²€ì¦ ì‹œì‘\n');
    
    validation_results = struct();
    model_performance = struct();
    
    try
        % 1. K-Fold êµì°¨ ê²€ì¦
        fprintf('  ğŸ”„ K-Fold êµì°¨ ê²€ì¦ ì‹¤í–‰ ì¤‘...\n');
        k_folds = 5;
        [cv_scores, cv_models] = performKFoldCrossValidation(training_data, k_folds, simulation_state);
        
        validation_results.cross_validation = cv_scores;
        validation_results.cv_models = cv_models;
        
        % 2. í™€ë“œì•„ì›ƒ ê²€ì¦
        fprintf('  ğŸ“Š í™€ë“œì•„ì›ƒ ê²€ì¦ ì‹¤í–‰ ì¤‘...\n');
        [holdout_scores, holdout_models] = performHoldoutValidation(training_data, validation_data, simulation_state);
        
        validation_results.holdout_validation = holdout_scores;
        validation_results.holdout_models = holdout_models;
        
        % 3. ì‹œê°„ ì‹œë¦¬ì¦ˆ ê²€ì¦ (ì‹œê°„ ìˆœì„œ ê³ ë ¤)
        fprintf('  â° ì‹œê°„ ì‹œë¦¬ì¦ˆ ê²€ì¦ ì‹¤í–‰ ì¤‘...\n');
        [time_series_scores, ts_models] = performTimeSeriesValidation(training_data, simulation_state);
        
        validation_results.time_series_validation = time_series_scores;
        validation_results.ts_models = ts_models;
        
        % 4. ì¢…í•© ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        model_performance = calculateOverallPerformance(validation_results);
        
        fprintf('  âœ… ëª¨ë¸ ê²€ì¦ ì™„ë£Œ\n');
        fprintf('    - CV í‰ê·  ì ìˆ˜: %.3f\n', mean([cv_scores.r2_scores]));
        fprintf('    - í™€ë“œì•„ì›ƒ ì ìˆ˜: %.3f\n', holdout_scores.r2_score);
        fprintf('    - ì‹œê³„ì—´ ì ìˆ˜: %.3f\n', time_series_scores.r2_score);
        
    catch ME
        fprintf('  âŒ ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: %s\n', ME.message);
        validation_results = [];
        model_performance = [];
    end
end

function [cv_scores, cv_models] = performKFoldCrossValidation(data, k_folds, simulation_state)
    % K-Fold êµì°¨ ê²€ì¦ êµ¬í˜„
    
    n_samples = size(data.features, 1);
    fold_size = floor(n_samples / k_folds);
    
    cv_scores = struct();
    cv_scores.r2_scores = zeros(1, k_folds);
    cv_scores.mse_scores = zeros(1, k_folds);
    cv_scores.mae_scores = zeros(1, k_folds);
    
    cv_models = cell(1, k_folds);
    
    for fold = 1:k_folds
        fprintf('    Fold %d/%d ì‹¤í–‰ ì¤‘...\n', fold, k_folds);
        
        % ê²€ì¦ ì„¸íŠ¸ ì¸ë±ìŠ¤
        val_start = (fold - 1) * fold_size + 1;
        val_end = min(fold * fold_size, n_samples);
        val_indices = val_start:val_end;
        
        % í›ˆë ¨ ì„¸íŠ¸ ì¸ë±ìŠ¤
        train_indices = setdiff(1:n_samples, val_indices);
        
        % ë°ì´í„° ë¶„í• 
        train_features = data.features(train_indices, :);
        train_targets = data.targets(train_indices, :);
        val_features = data.features(val_indices, :);
        val_targets = data.targets(val_indices, :);
        
        % ëª¨ë¸ í›ˆë ¨
        fold_model = trainMLModels(train_features, train_targets, simulation_state);
        cv_models{fold} = fold_model;
        
        % ì˜ˆì¸¡ ë° í‰ê°€
        predictions = predictWithModel(fold_model, val_features);
        
        cv_scores.r2_scores(fold) = calculateR2Score(val_targets, predictions);
        cv_scores.mse_scores(fold) = mean((val_targets - predictions).^2);
        cv_scores.mae_scores(fold) = mean(abs(val_targets - predictions));
    end
    
    % í†µê³„ ìš”ì•½
    cv_scores.mean_r2 = mean(cv_scores.r2_scores);
    cv_scores.std_r2 = std(cv_scores.r2_scores);
    cv_scores.mean_mse = mean(cv_scores.mse_scores);
    cv_scores.mean_mae = mean(cv_scores.mae_scores);
end

function r2 = calculateR2Score(y_true, y_pred)
    % RÂ² ì ìˆ˜ ê³„ì‚°
    
    ss_res = sum((y_true - y_pred).^2);
    ss_tot = sum((y_true - mean(y_true)).^2);
    
    if ss_tot == 0
        r2 = 1;  % ì™„ë²½í•œ ì˜ˆì¸¡
    else
        r2 = 1 - ss_res / ss_tot;
    end
end

function model = trainMLModels(features, targets, simulation_state)
    % í†µí•© ML ëª¨ë¸ í›ˆë ¨
    
    model = struct();
    
    % ì‹ ê²½ë§ í›ˆë ¨
    model.neural_network = trainNeuralNetwork(features, targets(:, 1), simulation_state);
    
    % SVM í›ˆë ¨
    model.svm = trainSVM(features, targets(:, 2), simulation_state);
    
    % ëœë¤ í¬ë ˆìŠ¤íŠ¸ í›ˆë ¨
    model.random_forest = trainRandomForest(features, targets(:, 3), simulation_state);
end
```

### 9.3.2 Feature Importance Analysis

```matlab
function feature_importance = analyzeFeatureImportance(trained_models, feature_names, simulation_state)
    % íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
    
    feature_importance = struct();
    n_features = length(feature_names);
    
    % 1. ìˆœì—´ ì¤‘ìš”ë„ (Permutation Importance)
    permutation_importance = calculatePermutationImportance(trained_models, simulation_state);
    
    % 2. ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë‚´ì¥ ì¤‘ìš”ë„
    if isfield(trained_models, 'random_forest')
        rf_importance = trained_models.random_forest.feature_importance;
    else
        rf_importance = ones(n_features, 1) / n_features;
    end
    
    % 3. ê·¸ë˜ë””ì–¸íŠ¸ ê¸°ë°˜ ì¤‘ìš”ë„ (ì‹ ê²½ë§)
    if isfield(trained_models, 'neural_network')
        gradient_importance = calculateGradientImportance(trained_models.neural_network, simulation_state);
    else
        gradient_importance = ones(n_features, 1) / n_features;
    end
    
    % 4. ì¢…í•© ì¤‘ìš”ë„ ê³„ì‚°
    weights = [0.4, 0.3, 0.3];  % ìˆœì—´, RF, ê·¸ë˜ë””ì–¸íŠ¸
    combined_importance = weights(1) * permutation_importance + ...
                         weights(2) * rf_importance + ...
                         weights(3) * gradient_importance;
    
    % ì •ê·œí™”
    combined_importance = combined_importance / sum(combined_importance);
    
    % ê²°ê³¼ êµ¬ì¡°ì²´ ìƒì„±
    feature_importance.permutation = permutation_importance;
    feature_importance.random_forest = rf_importance;
    feature_importance.gradient = gradient_importance;
    feature_importance.combined = combined_importance;
    feature_importance.feature_names = feature_names;
    
    % ìƒìœ„ ì¤‘ìš” íŠ¹ì„± ì¶œë ¥
    [sorted_importance, sort_idx] = sort(combined_importance, 'descend');
    fprintf('\nğŸ“Š íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ê²°ê³¼:\n');
    for i = 1:min(10, length(feature_names))
        fprintf('  %d. %s: %.3f\n', i, feature_names{sort_idx(i)}, sorted_importance(i));
    end
end

function permutation_importance = calculatePermutationImportance(models, simulation_state)
    % ìˆœì—´ ì¤‘ìš”ë„ ê³„ì‚°
    
    if ~isfield(simulation_state, 'validation_data')
        permutation_importance = ones(15, 1) / 15;  % ê¸°ë³¸ê°’
        return;
    end
    
    val_data = simulation_state.validation_data;
    baseline_score = evaluateModel(models, val_data.features, val_data.targets);
    
    n_features = size(val_data.features, 2);
    permutation_importance = zeros(n_features, 1);
    
    for feature_idx = 1:n_features
        % íŠ¹ì„± ìˆœì—´
        shuffled_features = val_data.features;
        shuffled_features(:, feature_idx) = shuffled_features(randperm(size(shuffled_features, 1)), feature_idx);
        
        % ìˆœì—´ëœ ë°ì´í„°ë¡œ í‰ê°€
        shuffled_score = evaluateModel(models, shuffled_features, val_data.targets);
        
        % ì¤‘ìš”ë„ = ì„±ëŠ¥ ê°ì†ŒëŸ‰
        permutation_importance(feature_idx) = max(0, baseline_score - shuffled_score);
    end
    
    % ì •ê·œí™”
    if sum(permutation_importance) > 0
        permutation_importance = permutation_importance / sum(permutation_importance);
    else
        permutation_importance = ones(n_features, 1) / n_features;
    end
end
```

### 9.3.3 Model Ensemble and Meta-Learning

```matlab
function ensemble_model = createModelEnsemble(individual_models, validation_data, simulation_state)
    % ëª¨ë¸ ì•™ìƒë¸” ìƒì„±
    
    fprintf('ğŸ­ ëª¨ë¸ ì•™ìƒë¸” êµ¬ì„± ì¤‘...\n');
    
    ensemble_model = struct();
    ensemble_model.models = individual_models;
    
    % 1. ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
    model_names = fieldnames(individual_models);
    n_models = length(model_names);
    model_scores = zeros(n_models, 1);
    
    for i = 1:n_models
        model_name = model_names{i};
        model = individual_models.(model_name);
        predictions = predictWithModel(model, validation_data.features);
        model_scores(i) = calculateR2Score(validation_data.targets, predictions);
        
        fprintf('  %s ì„±ëŠ¥: RÂ² = %.3f\n', model_name, model_scores(i));
    end
    
    % 2. ê°€ì¤‘ì¹˜ ê³„ì‚° (ì„±ëŠ¥ ê¸°ë°˜)
    % ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ ê³„ì‚°
    exp_scores = exp(model_scores * 5);  % ì˜¨ë„ íŒŒë¼ë¯¸í„° 5
    ensemble_weights = exp_scores / sum(exp_scores);
    
    ensemble_model.weights = ensemble_weights;
    ensemble_model.model_names = model_names;
    
    % 3. ìŠ¤íƒœí‚¹ ë©”íƒ€ ëª¨ë¸ í›ˆë ¨ (ì„ íƒì‚¬í•­)
    if simulation_state.config.use_stacking
        meta_model = trainStackingMetaModel(individual_models, validation_data, simulation_state);
        ensemble_model.meta_model = meta_model;
        ensemble_model.use_stacking = true;
    else
        ensemble_model.use_stacking = false;
    end
    
    % 4. ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€
    ensemble_predictions = predictWithEnsemble(ensemble_model, validation_data.features);
    ensemble_score = calculateR2Score(validation_data.targets, ensemble_predictions);
    
    ensemble_model.ensemble_score = ensemble_score;
    
    fprintf('  ğŸ¯ ì•™ìƒë¸” ì„±ëŠ¥: RÂ² = %.3f\n', ensemble_score);
    fprintf('  ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ: %.3f\n', ensemble_score - max(model_scores));
end

function predictions = predictWithEnsemble(ensemble_model, features)
    % ì•™ìƒë¸” ì˜ˆì¸¡
    
    if ensemble_model.use_stacking
        % ìŠ¤íƒœí‚¹ ë°©ë²•
        base_predictions = zeros(size(features, 1), length(ensemble_model.model_names));
        
        for i = 1:length(ensemble_model.model_names)
            model_name = ensemble_model.model_names{i};
            model = ensemble_model.models.(model_name);
            base_predictions(:, i) = predictWithModel(model, features);
        end
        
        predictions = predictWithModel(ensemble_model.meta_model, base_predictions);
        
    else
        % ê°€ì¤‘ í‰ê·  ë°©ë²•
        predictions = zeros(size(features, 1), 1);
        
        for i = 1:length(ensemble_model.model_names)
            model_name = ensemble_model.model_names{i};
            model = ensemble_model.models.(model_name);
            model_predictions = predictWithModel(model, features);
            
            predictions = predictions + ensemble_model.weights(i) * model_predictions;
        end
    end
end
```

---

*Chapter 9ëŠ” SFDP v17.3ì˜ ë¨¸ì‹ ëŸ¬ë‹ êµ¬í˜„ì˜ í•µì‹¬ì„ ë‹¤ë£¹ë‹ˆë‹¤. ì‹ ê²½ë§, SVM, ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë“± ë‹¤ì–‘í•œ ML ì•Œê³ ë¦¬ì¦˜ì„ í†µí•©í•œ ê²½í—˜ì  ëª¨ë¸ ìŠ¤ìœ„íŠ¸, ì •êµí•œ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§, êµì°¨ ê²€ì¦ í”„ë ˆì„ì›Œí¬, íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„, ëª¨ë¸ ì•™ìƒë¸” ë“±ì„ í†µí•´ ë¬¼ë¦¬ ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜ì„ ë³´ì™„í•˜ëŠ” ê°•ë ¥í•œ ë°ì´í„° ê¸°ë°˜ ì˜ˆì¸¡ ì‹œìŠ¤í…œì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”ì™€ ì‹ ë¢°ë„ í‰ê°€ë¥¼ í†µí•´ ì˜ˆì¸¡ ê²°ê³¼ì˜ í’ˆì§ˆì„ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.*